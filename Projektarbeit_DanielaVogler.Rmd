---
title: "Detektion von Sepsiserkrankungen in Intensivpatienten"
subtitle: "Projektarbeit im Zertifikatsprogramm \"Medical Data Science\""
author: "Daniela Vogler"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    keep_tex: yes
header-includes:
  - \renewcommand{\tablename}{Tabelle}
  - \renewcommand{\contentsname}{Inhalt}
  - \renewcommand{\figurename}{Abb.}
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage[utf8]{inputenc}
  - \usepackage{makecell}
  - \usepackage{xcolor}
  - \usepackage{hyphenat}
  - \hyphenation{Ent-wicklung Event-rate Intensiv-aufent-halts er-reicht
        Intensiv-station Intensiv-stationen Visua-lisierung er-klärender
        Datenaufberei-tung}
  - \floatplacement{figure}{H}
bibliography: bibliography.bib
csl: din-1505-2.csl
fontsize: 12pt
editor_options: 
  markdown: 
  wrap: 80
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.pos = "H",
	warning = FALSE,
	cache = TRUE,
	cache.comments = FALSE,
	include = FALSE
)
```

```{r requirements, include = FALSE, cache = FALSE}
library(tidyverse)
library(here)
library(kableExtra)
library(grid)
library(gridExtra)
library(reshape)
library(rpart)
library(rpart.plot)
library(caret)
library(MLmetrics)
library(randomForest)
library(ranger)
library(gbm)
library(naniar)
library(ConfusionTableR)
library(png)
library(randomForestExplainer)
library(DALEX)
```

\newpage

# Einleitung

Eine Sepsis ist eine sehr ernste gesundheitliche Komplikation, insbesondere auf 
Intensivstationen. Die Sterblichkeitsrate liegt bei etwa 30-50 %. Zudem entstehen 
jährlich Behandlungskosten in Höhe von mehreren Milliarden Euro alleine in 
Deutschland [@Fleischmann].
\newline
Eine frühzeitige Erkennung und damit einhergehend eine rechtzeitige Therapie gelten
als wichtigste Mittel zur Verbesserung der Überlebensrate. Jedoch sind die Symptome
vor allem in der frühen Erkrankungsphase unspezifisch und vielfältig, was eine
rechtzeitige Diagnose erschwert.

In dieser Arbeit werden daher Möglichkeiten untersucht, eine Sepsis anhand
von Blutwerten und Vitalparametern, die im Rahmen von Intensivbehandlungen 
teils regelmäßig, teils nach Bedarf erhoben werden, mittels prädiktiver Modelle 
bereits zu erkennen, bevor sie klinisch diagnostiziert wird. Da der Einsatz von
Machine Learning-Algorithmen zur medizinischen Entscheidungsfindung nicht überall 
möglich und gewünscht ist, wird neben der Entwicklung von Vorhersagemodellen
auch die Frage untersucht, ob aus den Modellen Entscheidungsregeln zur nicht
maschinellen Unterstützung ärztlicher Entscheidungen abzulesen sind.

Die bearbeitete Fragestellung lässt sich wie folgt zusammenfassen: 
\newline
1. Welche Entscheidungskriterien hinsichtlich einer Klassifizierung von
Intensivpatienten in die Kategorien "Sepsispatient" und "Nicht-Sepsispatient"
lassen sich mittels einzelner Entscheidungsbäume ableiten?
\newline
2. Welches baumbasierte Modell ist am besten geeignet, um aus demografischen 
Daten, Blutwerten und Vitalparametern eines Intensivpatienten vorherzusagen, ob
dieser an einer Sepsis erkrankt ist beziehungsweise gerade dabei ist, eine Sepsis
zu entwickeln?
\newline
Für dieses binäre Klassifizierungsproblem werden exemplarisch CARTs
(Entscheidungsbäume), Random Forests (Bagging) und GBMs (Boosting) untersucht.

Die verwendeten Daten stammen aus dem unter *Open Database License (ODbL)* 
verfügbaren Kaggle-Datensatz *Dataset.csv* [@Binitagiri; @Reyna]. 

\newpage

# Datenexploration und -bereinigung

```{r loading data}
data_complete <- read.csv(here("data/Dataset.csv"))
```

```{r loading variable description}
var_info <- read.csv2(
  here("data/Datensatzbeschreibung.csv")
  )
```

```{r subsampling}
# Subsample ziehen, um die Datenmenge besser handhaben zu können
set.seed(12345)

n <- round(0.1*length(unique(data_complete$Patient_ID)))

data_sample_index <- data_complete %>% 
  distinct(Patient_ID) %>% 
  sample_n(n)

data_sample <- data_complete %>% 
  filter(Patient_ID %in% data_sample_index$Patient_ID)
```

Für diese Arbeit wird ein zufälliges Subsample des oben angegebenen Datensatzes 
mit 
`r prettyNum(nrow(data_sample), big.mark = ".", decimal.mark = ",")` 
Beobachtungen zu 
`r prettyNum(length(unique(data_sample$Patient_ID)), big.mark = ".", 
              decimal.mark = ",")` 
Intensivpatienten aus zwei verschiedenen US-amerikanischen Krankenhäusern 
verwendet. Die Daten umfassen neben einigen demografischen und administrativen
Basisangaben zum Fall acht Vitalparameter und 26 Blutwerte, siehe Tabelle
\ref{vars_table} im Anhang. 
\newline
Die Erfassung erfolgte stündlich; kontinuierlich gemessene Werte wurden gemittelt.

Das Labeling erfolgte in Übereinstimmung mit der Sepsis-3-Definition.
\newline
Zu beachten ist, dass die Daten bereits sechs Stunden vor dem Ausbruch der Sepsis
als positiv gelabelt wurden, sodass eine korrekte Klassifizierung als 
Sepsispatient einem zeitlichen Vorsprung von drei bis sechs Stunden entspricht. 
Dabei ergibt sich ein Zeitfenster kleiner sechs Stunden, wenn der Ausbruch 
innerhalb der ersten sechs Stunden nach Aufnahme auf die Intensivstation erfolgt,
wobei Ausbrüche innerhalb weniger als vier Stunden aus den Daten entfernt wurden.
Als Ausbruchszeitpunkt gilt hierbei das Minimum von
\newline
- dem Zeitpunkt des Auftretens einer Organdysfunktion gemäß Sequential Organ
Failure Assessment (SOFA) 
\newline
- dem Zeitpunkt, zu dem sich ein konkreter klinischer Verdacht auf eine Infektion
in einer intravenösen Antibiotikagabe und dem Anlegen von Blutkulturen
widerspiegelt. 
\newline Zu den Details siehe [@Reyna].

```{r data structure}
summary(data_sample)
str(data_sample)

# Outcome pro Patient abspeichern
outcome <- data_sample %>% 
  group_by(Patient_ID) %>% 
  mutate(Sepsis = max(SepsisLabel)) %>% 
  ungroup() %>% 
  select(Patient_ID, Sepsis) %>% 
  unique()

table(outcome$Sepsis)
```

Bei Betrachtung der Zusammenfassung des Datensatzes fielen zwei Fehler auf, die
wie folgt bereinigt wurden:

1. *FiO2* (inspiratorische Sauerstofffraktion) drückt einen Anteil aus und muss 
daher zwischen 0 und 1 liegen. Werte außerhalb dieses Bereichs wurden deshalb 
entfernt.

2. *HospAdmTime*, die Zeit zwischen Hospitalisierung und Verlegung auf die 
Intensivstation (ITS), ist je nach Berechnung entweder positiv oder negativ, 
kann aber nicht sowohl positive als auch negative Werte annehmen, da niemand auf
die Intensivstation verlegt werden kann, bevor er überhaupt im Krankenhaus ist.
Es ist daher naheliegend, dass unterschiedliche Berechnungen 
(Aufnahmezeitpunkt_gesamt - Aufnahmezeitpunkt_Intensiv und
Aufnahmezeitpunkt_Intensiv - Aufnahmezeitpunkt_gesamt) vorgenomen wurden. 
Hier wurde auf positive Vorzeichen vereinheitlicht, da diese für die 
Interpretation geeigneter sind ("Zeit, die zwischen Aufnahme im Krankenhaus und
Aufnahme auf der ITS vergangen ist").

```{r data cleaning}
data_sample %>%
  filter(FiO2 < 0 | FiO2 > 1)

data_sample %>%
  filter(HospAdmTime > 0) %>% 
  distinct(Patient_ID, HospAdmTime)

data_sample <- data_sample %>% 
  mutate(FiO2 = case_when(FiO2 >= 0 & FiO2 <= 1 ~ FiO2),
         HospAdmTime = case_when(HospAdmTime < 0 ~ -1*HospAdmTime,
                                 TRUE ~ HospAdmTime)
         )
```

```{r data preparation}
data_sample <- data_sample %>% 
  mutate(Gender = factor(Gender, levels = c(0, 1), labels = c("W", "M")),
         Unit1 = factor(Unit1, levels = c(0, 1), labels = c("nein", "ja")),
         Unit2 = factor(Unit2, levels = c(0, 1), labels = c("nein", "ja")),
         SepsisLabel = factor(SepsisLabel, levels = c(1,0), 
                              labels = c("positiv", "negativ")
                              )
  )

outcome <- outcome %>% 
  mutate(Sepsis = factor(Sepsis, levels = c(1, 0),
                         labels = c("positiv", "negativ")
                        )                                
         )  # "positiv" muss 1. Klasse sein, damit caret später die Gütemaße 
            # geeignet zuordnet            

# Zur weiteren Verwendung wird jedem Datensatz der Outcome über den gesamten Zeitraum hinzugefügt
data_sample <- merge(data_sample, outcome, by = "Patient_ID")
```

Nun analysieren wir die Daten auf Korrelationen zwischen den numerischen Variablen
und betrachten dazu eine Heatmap der Korrelationskoeffizienten nach Pearson:

```{r data correlation, fig.cap="\\label{cor_plot}Korrelationskoeffizienten nach Pearson", fig.height=8, fig.width=12, message=FALSE, warning=FALSE, include = TRUE}
data_sample %>% 
  select_if(is.numeric) %>%  
  select(-Patient_ID, -X) %>% 
  GGally::ggcorr(method = c("pairwise.complete.obs", "pearson"),
         label = TRUE, label_size = 2, label_alpha = TRUE) 
```

Baumbasierte Methoden funktionieren auch bei starker Multikollinearität gut, da 
sukzessive einzelne Variablen für die Splits herangezogen werden. Liefert eine 
Variable keinen oder kaum Informationsgewinn, weil sie stark mit einer vorher 
bereits zum Split genutzten Variable korreliert, wird sie nicht verwendet. Daher
müssen stark korrelierende Prädiktoren nicht unbedingt von der Analyse
ausgeschlossen werden. Allerdings sollten starke Kollinearitäten später bei der
Interpretation der Entscheidungskriterien berücksichtigt werden. Es ist möglich,
dass nicht die eigentlich relevante Variable für einen Split herangezogen wird,
sondern eine stark mit ihr korrelierende.

Bei Vorliegen eines vollständigen linearen Zusammenhangs 
(Korrelationskoeffizient +/- 1) soll dennoch eine Variablenselektion erfolgen:

*Hour* dient nur zur Information und soll nicht als Einflussvariable verwendet
werden. Daher kann sie hier vernachlässigt werden.

```{r Bilirubin}
data_sample %>% 
  filter(is.na(Bilirubin_total) & !is.na(Bilirubin_direct)) %>% 
  summarize(n())

data_sample %>% 
  filter(is.na(Bilirubin_direct) & !is.na(Bilirubin_total)) %>% 
  summarize(n())
```

Da es nur sehr wenige Beobachtungen gibt, in denen das Gesamt-Bilirubin fehlt, 
aber direkts Bilirubin gemessen wurde, andersherum jedoch einige Beobachtungen 
existieren, die eine Messung des Gesamt-Bilirubins enthalten, aber keine des 
direkten Bilirubins, wird die Variable *Bilirubin_direct* aus dem Datensatz
entfernt.

```{r Hct Hgb}
data_sample %>% 
  filter(is.na(Hct) & !is.na(Hgb)) %>% 
  summarize(n())

data_sample %>% 
  filter(is.na(Hgb) & !is.na(Hct)) %>% 
  summarize(n())
```

Hämoglobin- und Hämatokrit-Wert entsprechen einander ungefähr, weil die
Erythrozyten den Großteil des Gesamtvolumens der Blutzellen ausmachen
[@BMSGPK]. Auch hier wird daher nach Vollständigkeit der Beobachtungen entschieden
und die Variable *Hgb* aus dem Datensatz entfernt.

```{r variable selection}
data_sample <- data_sample %>% 
  select(-Bilirubin_direct, -Hgb)
```

Bei Betrachtung der Boxplots der möglichen numerischen Einflussfaktoren fällt 
keine Variable durch besonders deutliche Unterschiede zwischen Patienten mit und 
ohne Sepsis auf.

Da schwerwiegende Erkrankungen durchaus extreme medizinische Werte hervorbringen 
können, ist es schwierig festzulegen, wann Ausreißer Messfehler sind und daher 
aus der Betrachtung herausgenommen werden sollten. Zudem sind baumbasierte 
Methoden robust gegenüber Ausreißern. Deshalb werden hier keine weiteren 
Beobachtungen entfernt.

```{r boxplots, include = TRUE, warning = FALSE, fig.height=8, fig.width=10, fig.cap="Boxplots der numerischen Einflussfaktoren"}
# Numerische Einflussfaktoren in Bezug zum temporären Sepsisstatus
melt_sample <- data_sample %>% 
  select(-Gender, -Unit1, -Unit2, ) %>% 
  melt(., id.vars = c("Patient_ID", "X", "Hour", "SepsisLabel", "Sepsis"))

p <- ggplot(melt_sample, aes(factor(variable), value)) + 
  geom_boxplot(aes(fill = SepsisLabel)) + 
  labs(x = "", y = "") +
  facet_wrap(~variable, scale = "free") +
  theme(strip.background = element_blank(), strip.text = element_blank())

p

rm(melt_sample, p)
```

Da Temperatur und Blutwerte häufig nur einmal täglich bestimmt werden, werden 
diese bei fehlenden Werten bis zu 24 Stunden fortgeschrieben (LOCF).

```{r LOCF imputation, warnings = FALSE}
data_sample_imputed <- data_sample %>% 
  pivot_longer(cols = Temp | BaseExcess:Platelets, 
               names_to = "var", 
               values_to = "value") %>% 
  group_by(Patient_ID, var) %>% 
  mutate(last_Hour = max(Hour[!is.na(value)]), 
         diff = Hour - last_Hour) %>% 
  fill(value) %>% 
  mutate(value = replace(value, which(diff > 24), NA)) %>% 
  select(-last_Hour, -diff) %>% 
  pivot_wider(names_from = var, values_from = value) %>% 
  relocate(Temp, .after = O2Sat) %>% 
  relocate(BaseExcess:Platelets, .after = EtCO2) %>% 
  as.data.frame()
```

\newpage

# Methoden und Ergebnisse

Nach den grundlegenden Vorbereitungen aus dem vorigen Kapitel ergeben sich zwei 
wesentliche Herausforderungen aus der Datenstruktur.

Zum einen ist die Anzahl fehlender Werte in vielen Variablen sehr hoch. Dies
gilt auch nach der bereits aufgrund von fachlichen Überlegungen erfolgten 
LOCF-Imputation einiger Prädiktoren weiterhin. Während der CART-Algorithmus zum
Anpassen von Entscheidungsbäumen solche Situationen intern durch Surrogate Splits
handhabt, müssen für viele andere Varianten von Entscheidungsbäumen und
Ensemblemethoden entweder fehlende Daten imputiert oder der Datensatz auf
vollständige Beobachtungen eingeschränkt werden.Dies gilt zum Beispiel für die
Anwendung von Random Forests. Gradient Boosting Machines wiederum behandeln 
fehlende Werte intern, indem sie diese als eigene Klasse beziehungsweise Wert
definieren.
\newline
Der geeignete Umgang mit fehlenden Daten hängt stark davon ab, welcher Mechanismus
dem Fehlen zugrunde liegt. Imputationsmethoden setzen grundsätzlich mindestens
Missing At Random (MAR) voraus. Es ist anzunehmen, dass im vorliegenden Datensatz
tatsächlich einige Werte nur deshalb fehlen, weil bestimmte Messungen unabhängig
vom Gesundheitszustand des Patienten nur zu bestimmten Zeiten beziehungsweise in
größeren Abständen vorgenommen werden. Es ist aber auch durchaus wahrscheinlich,
dass manche Messungen (insbesondere die Bestimmung spezieller Blutwerte) nur 
bei Vorliegen konkreter Verdachtsmomente erfolgen und somit ein Zusammenhang des
Fehlens mit dem tatsächlichen Wert (und eventuell auch mit der Wahrscheinlichkeit
einer Sepsis) besteht. Ursachenforschung gemeinsam mit den ärztlichen und
pflegerischen Fachkräften wäre an dieser Stelle angeraten, ist im Rahmen dieser
Arbeit aber nicht möglich, da die Analyse retrospektiv und ohne Kooperation mit 
den Daten generierenden Krankenhäusern erfolgt. Im Folgenden werden, sofern für 
die jeweiligen Modelle erforderlich, zwei Vorgehensweisen miteinander verglichen
- eine Analyse vollständiger Fälle (nach Einschränkung auf relativ gut gefüllte
Variablen) und eine Imputation mit der im R-Paket *randomForest* implementierten 
Methode nach Breiman, die fehlende Werte iterativ anhand von Proximity-Matrizen
in Random Forests konstruiert [@Breiman]. Auf multiple Imputation wird verzichtet,
um den anfangs gezielt verkleinerten Datensatz nicht wieder erheblich zu
vergrößern und weil auch diese Methode MAR voraussetzt, sodass die vermutete 
hauptsächliche Schwachstelle damit nicht behoben wäre. Dennoch ist festzuhalten,
dass multiple Imputation eine mögliche Alternative darstellt, die hinsichtlich
einer eventuellen Verzerrung Vorteile aufweisen kann.

Zum anderen müssen die longitudinalen Daten für die Anwendung baumbasierter 
Verfahren zu je einer Beobachtung pro Patient zusammengefasst werden. Dazu werden 
drei Ansätze miteinander verglichen:

**1. Baseline-Datensatz:** Es werden alle Datensätze mit *Hour* = 1, d. h. die
zweite vorliegende Messung, verwendet. Die Wahl fällt deshalb nicht auf die erste
Messung, weil in dieser besonders viele Werte fehlen. Es ist zu beachten, dass der
Zeitpunkt der ersten Messung nicht notwendigerweise mit der Aufnahme auf die
Intensivstation zusammenfällt.
\newline Dieses Vorgehen entspricht einer einmaligen Beurteilung der Patienten zu 
Beginn der Intensivbehandlung bzw. zum erstmöglichen Zeitpunkt.

**2. Onset-Datensatz:** Für Sepsispatienten wird die erste mit einem positiven 
Outcome gelabelte Beobachtung verwendet. Für Nicht-Sepsispatienten wird eine 
Beobachtung zufällig ausgewählt, da nicht bekannt ist, wann ein solcher Patient 
der Entwicklung einer Sepsis am nächsten war.
\newline Dieser Ansatz simuliert eine regelmäßige Klassifizierung liegender 
Intensivpatienten anhand ihrer jeweils aktuellen klinischen Parameter.

**3. Regressionsdatensatz:** Für jeden Patienten wird mittels linearer Regression 
die Entwicklung jeder zeitabhängigen Prädiktorvariable modelliert und der
patientenindividuelle Slope als Prädiktor verwendet. Dabei wird für
Nicht-Sepsispatienten der gesamte Beobachtungszeitraum einbezogen und für
Sepsispatienten der Zeitraum bis zum ersten positiven Outcome.
\newline Dies entspricht einer regelmäßigen Klassifizierung der Patienten anhand 
aller ihrer bis zu diesem Zeitpunkt vorliegenden Parameter.

Die drei entstandenen Datensätze werden anschließend jeweils in einen Trainings-
und Validierungsdatensatz mit 75 % der Beobachtungen und einen Testdatensatz mit
den restlichen 25 % der Beobachtungen aufgeteilt.

```{r creating baseline dataset}
# Es sollen die Messwerte zur Stunde 1 verwendet werden, aber für den Outcome
# ist der Gesamtzeitraum entscheidend
dataset1 <- data_sample_imputed %>%
  filter(Hour == 1) %>%
  select(-SepsisLabel)
```

```{r creating onset dataset}
# Es sollen die Beobachtungen zum Zeitpunkt des ersten positiven Labels verwendet
# werden, für Nicht-Sepsispatienten ein zufälliger Zeitpunkt
data_pos <- data_sample_imputed %>%
  filter(SepsisLabel == "positiv") %>%
  group_by(Patient_ID) %>%
  mutate(ersteSepsisflag = min(Hour)) %>%
  ungroup() %>%
  filter(Hour == ersteSepsisflag) %>%
  select(-ersteSepsisflag)

tmp_data_neg <- data_sample_imputed %>%
  filter(Sepsis == "negativ") %>%
  group_by(Patient_ID) %>%
  summarize(max_Hour = max(Hour),
            selected_Hour = sample(c(1:max_Hour),
                                    size = 1, replace = F)
            )

data_neg <- data_sample_imputed %>%
  filter(Sepsis == "negativ") %>%
  merge(., tmp_data_neg, by = "Patient_ID") %>%
  filter(Hour == selected_Hour) %>%
  select(-max_Hour, -selected_Hour)


dataset2 <- union_all(data_pos, data_neg) %>%
  select(-SepsisLabel) %>%
  as.data.frame()

rm(data_pos, data_neg, tmp_data_neg)
```

```{r creating regression dataset}
# Anstelle der Vital- und Laborwerte sollen die patientenindividuellen Slopes
# verwendet werden. Wenn ein Patient eine Sepsis entwickelt, wird nur der Zeitraum 
# bis zum ersten positiven Label betrachtet.

patient_model <- function(df) {
  if (sum(!is.na(df$yvalue)) < 2) NA
  else lm(yvalue ~ Hour, data = df)$coeff[2]
  }

data_pos <- data_sample_imputed %>%
  filter(SepsisLabel == "positiv") %>%
  group_by(Patient_ID) %>%
  mutate(ersteSepsisflag = min(Hour),
         ICULOS_end = min(ICULOS)) %>%
  distinct(Patient_ID, ersteSepsisflag, ICULOS_end)

data_sub <- data_sample_imputed %>%
  merge(., data_pos, by = "Patient_ID", all.x = TRUE) %>%
  filter(Sepsis == "negativ" |
           (ersteSepsisflag > 0 &  Hour <= ersteSepsisflag)
         # für ersteSepsisFlag = 0 lässt sich keine Entwicklung aufzeigen
         ) %>%
  group_by(Patient_ID) %>%
  mutate(ICULOS = case_when(!is.na(ICULOS_end) ~ ICULOS_end,
                            TRUE ~ max(ICULOS))) %>%
  ungroup() %>%
  pivot_longer(cols = HR:Platelets,
               names_to = "yvar",
               values_to = "yvalue") %>%
  mutate(yvar = paste("slope_", yvar, sep = "")) # eindeutige Namen erzeugen

by_patient <- data_sub %>%
  group_by(Patient_ID, yvar, ICULOS) %>% # damit ICULOS erhalten bleibt
  nest() %>%
  mutate(slope = map(data, patient_model)) %>%
  unnest(slope) %>%
  select(-data) %>%
  pivot_wider(names_from = yvar, values_from = slope)

dataset3 <- data_sample_imputed %>%
  distinct(Patient_ID, Age, Gender, Unit1, Unit2, HospAdmTime, Sepsis) %>%
  merge(by_patient, by = "Patient_ID", all = FALSE) %>%
  select(Patient_ID, Sepsis, everything())

rm(by_patient, data_sub, data_pos)
```

```{r train and test sets}
set.seed(12345)

# caret::createDataPartition balanciert die Outcome-Varaible aus
# der train_index für Datensatz 1 kann auch für Datensatz 2 genutzt werden (gleiche
# Patienten, gleicher Outcome)
train_index1 <- createDataPartition(dataset1$Sepsis, p = 0.75, list = FALSE)

# Datensatz 3 benötigt einen anderen train_index, da hier nicht alle Patienten 
# enthalten sind
train_index3 <- createDataPartition(dataset3$Sepsis, p = 0.75, list = FALSE)

train1 <- dataset1[train_index1,]
test1  <- dataset1[-train_index1,]
train2 <- dataset2[train_index1,]
test2  <- dataset2[-train_index1,]
train3 <- dataset3[train_index3,]
test3  <- dataset3[-train_index3,]
```

In den nächsten Abschnitten werden Entscheidungsbäume und Ensembles für jeweils
einen Datensatz angepasst. Das Tuning der Parameter und der Modellvergleich 
erfolgen mittels des R-Pakets *caret*.

Es wird eine 5-fache Kreuzvalidierung und aufgrund der sehr unausgewogenen Daten
(Eventrate `r prettyNum(
  round(
  length(unique((subset(data_sample, Sepsis == "positiv"))$Patient_ID)) /
  length(unique(data_sample$Patient_ID))   *100,
  digits = 1),
  decimal.mark = ",", big.mark = ".")` %) ein Upsampling vorgenommen.

Bei der Wahl der Evaluationsmetrik ist zu berücksichtigen, dass sowohl
Sensitivität als auch Spezifität eine hohe Relevanz für die Fragestellung haben. 
Je höher die Sensitivität, desto mehr Patienten können frühzeitig behandelt
werden, je niedriger die Spezifität, desto mehr Patienten werden möglicherweise
aufgrund der Klassifizierung unnötigen Therapien unterzogen. Eine häufige Wahl 
ist in solchen Situationen die *Area under the Curve (ROC)*. Für stark
unausgewogene Daten (mit niedriger Eventrate) sind aber Metriken, die auf 
Precision und Recall beruhen, von Vorteil. Daher bieten sich hier die 
*Area under the Precision Recall Curve (AUC)* und die F1-Metrik (harmonisches
Mittel aus Precision und Recall) an. [@Brownlee]

Wir werden später beim Modellvergleich sehen, dass die AUC-Metrik im Vergleich zur
F1-Metrik falsch positive Klassifizierungen stärker bestraft und dafür eine
niedrige Detektionsrate in Kauf nimmt. Für diese Arbeit ist die Entscheidung für
die F1-Metrik gefallen. Diese berücksichtigt gleichermaßen den Anteil der richtig
positiven an den Events als auch den Anteil der richtig positiven an den insgesamt
positiv klassifizierten Beobachtungen, kann aber leicht so verallgemeinert werden,
dass entweder der Recall (= Sensitivität) oder die Precision (deutsch: Relevanz)
stärker gewichtet wird. Eine flexible Anpassung beispielsweise im Rahmen einer
späteren klinischen Anwendung ist also gegeben.

```{r caret models control}
set.seed(12345)

ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE,
                     summaryFunction = prSummary,
                     sampling = "up")
```

## Der Baseline-Ansatz

Es wird ein Entscheidungsbaum mit dem CART-Algorithmus (R-Paket *rpart*) erstellt.
Da CART mit fehlenden Werten mittels Surrogate Splits automatisch umgehen kann,
ist kein Preprocessing notwendig. Das Tuning erfolgt mit den
*caret*-Standardeinstellungen, d. h. es wird ein Tunegrid der Länge drei für den
Complexity-Parameter *cp* basierend auf der *rpart*-Implementierung verwendet.

```{r cart model ds1}
set.seed(12345)

cart_ds1 <- train(x = train1[,4:41],
                  y = train1$Sepsis,
                  method = "rpart",
                  metric = "F",
                  trControl = ctrl)
```

Das beste Modell ergibt sich mit: 
`r knitr::kable(cart_ds1$bestTune, row.names = FALSE)`

Abbildung \ref{cart_ds1} zeigt den entstandenen Baum. In den Blättern sind die 
vorhergesagte Klasse, die zugehörige Wahrscheinlichkeit (d. h. der Anteil der 
korrekt klassifizierten Beobachtungen innerhalb des Blattes) und der prozentuale
Anteil der Beobachtungen in dem Blatt bezogen auf alle Beobachtungen des 
Trainingsdatensatzes angegeben.

```{r cart model results ds1, include = TRUE, fig.cap="\\label{cart_ds1}Entscheidungsbaum Baseline-Datensatz"}
rpart.plot(cart_ds1$finalModel, type = 5, extra = 108, box.palette = "RdBu")
```

Dass die Zeitspanne zwischen Krankenhausaufnahme und Verlegung auf die ITS
$\nobreak (*HospAdmTime*)$ eine Rolle spielt, erscheint einleuchtend. Da hier 
jedoch gar keine weiteren Variablen herangezogen werden, wird noch ein Baum ohne
*HospAdmTime* angepasst, um weitere Einblicke zu generieren. Dieser wird mit
Defaultparametern allerdings so tief, dass er nachträglich geprunt werden muss, 
um visuelle Erkenntnisse aus der Baumstruktur zu ermöglichen. Der geeignete
Complexity-Parameter wurde durch manuelles Ausprobieren bestimmt und auf cp = 0,02
gesetzt. Es ergibt sich:

```{r cart model adjusted ds1}
set.seed(12345)

cart_adj_ds1 <- train(x = train1[,-c(1,2,3,40,42)],
                  y = train1$Sepsis,
                  method = "rpart",
                  metric = "F",
                  trControl = ctrl)

cart_adj_ds1 <- prune(cart_adj_ds1$finalModel, cp = 0.02)
```

```{r cart model adjusted results ds1, include = TRUE, fig.cap="Entscheidungsbaum Baseline-Datensatz angepasst"}
rpart.plot(cart_adj_ds1, type = 5, extra = 108, box.palette = "RdBu")
```

Des Weiteren werden Random Forests trainiert. Da diese Methode nicht mit fehlenden
Werten funktioniert, wird zunächst die im R-Paket *randomForest* implementierte
Imputation nach Breiman angewandt [@Breiman]. Das Tuning erfolgt über folgendes
Tunegrid:

```{r tune grid rf, include = TRUE, echo = TRUE}
rf_grid <- expand.grid(
  .mtry = seq(2,35,3),
  .splitrule = "gini",
  .min.node.size = seq(5, 20, 5)
)
```

Die Anzahl der Bäume ist für diesen und alle weiteren Random Forests - basierend
auf einigen manuellen Tests - auf 100 festgelegt.

```{r rf model ds1}
set.seed(12345)

# Imputation nach Breiman:

train1_imputed <- rfImpute(train1$Sepsis ~ ., train1) %>%
  dplyr::rename(Sepsis = `train1$Sepsis`) %>%
  relocate(Sepsis, .after = ICULOS)

rf_ds1 <- train(x = train1_imputed[,4:41],
                y = train1$Sepsis,
                method = "ranger",
                tuneGrid = rf_grid,
                num.trees = 100,
                importance = "impurity",
                metric = "F",
                trControl = ctrl)
```

Das beste Modell hat folgende Parameter:
`r knitr::kable(rf_ds1$bestTune, row.names = FALSE)`

Als weiterer Ansatz wird nun der Datensatz auf diejenigen Prädiktoren mit weniger
als $\nobreak 30\ \%$ fehlenden Werten eingeschränkt und anschließend nur die
vollständigen Beobachtungen einbezogen. Dabei ist zu beachten, dass das Tunegrid 
für den Parameter *mtry* (d. h. die Anzahl der pro Split einbezogenen möglichen
Splitvariablen) auf maximal (Anzahl der Variablen - 1) beschränkt wird.

```{r non-missings ds1, include = TRUE, fig.cap="Fehlende Werte im Baseline-Datensatz"}
vis_miss(dataset1) +
  theme(axis.text.x =  element_text(angle = 90))
```

```{r rf non-missing ds1}
set.seed(12345)

train1_2 <- train1[, which(colMeans(!is.na(train1)) > 0.7)] 

# Entsprechend muss mtry im TuneGrid angepasst werden:
n_var1 <- train1_2 %>% 
  select(-Patient_ID, -X, -Hour, -Sepsis) %>% 
  ncol()
  
rf_grid1 <- expand.grid(
  .mtry = seq(2, n_var1 - 1 ,3),
  .splitrule = "gini",
  .min.node.size = seq(5, 20, 5)
)

rf_nm_ds1 <- train(Sepsis ~ . - Patient_ID - X - Hour - Sepsis,
                  data = train1_2,
                  method = "ranger",
                  na.action = na.omit,
                  tuneGrid = rf_grid1,
                  num.trees = 100,
                  importance = "impurity",
                  metric = "F",
                  trControl = ctrl)
``` 

 Das bestes Modell ergibt sich mit: 
`r knitr::kable(rf_nm_ds1$bestTune, row.names = FALSE)`

Während Random Forests Bäume durch Bagging kombinieren und damit insbesondere
die Korrelation zwischen den einzelnen Bäumen verringern, zielen auf Boosting
zurückgreifende Ensemble-Verfahren darauf, sukzessive die Vorhersagen für falsch
klassifizierte Beobachtungen zu verbessern. Hier wird nun zum Vergleich ein 
Stochastic Gradient Boosting mit dem R-Paket *gbm* durchgeführt. Dafür wird das 
Tunegrid wie folgt definiert:

```{r tune grid gbm, include = TRUE, echo = TRUE}
gbm_grid <-  expand.grid(interaction.depth = seq(1, 9, 2),
                        n.trees = c(100, 500, 1000),
                        shrinkage = c(0.001, 0.01, 0.05, 0.1),
                        n.minobsinnode = seq(5, 20, 5))
```

Obwohl, wie wir später sehen werden, in zwei Fällen die maximale hier getestete 
Anzahl von Bäumen zum besten Modell führt, wird auf höhere Werte für
*n.trees* verzichtet, um die Rechenzeit nicht zu sehr zu erhöhen und weil sich 
die Evaluationsmetriken nur noch geringfügig ändern.

```{r gbm model ds1}
set.seed(12345)

gbm_ds1 <- train(x = train1[,4:41],
                 y = train1$Sepsis,
                 method = "gbm",
                 distribution = "bernoulli", 
                 metric = "F",
                 tuneGrid = gbm_grid,
                 trControl = ctrl,
                 verbose = FALSE)
```

Das beste Modell ergibt sich mit folgenden Parametern:
`r knitr::kable(gbm_ds1$bestTune, row.names = FALSE)`

## Der Onset-Ansatz

Auf dem zweiten Datensatz werden die gleichen Arten von Modellen trainiert. Auch 
die Tunegrids bleiben mit Ausnahme der nötigen Anpassung von *mtry* bei
Verringerung der Variablenanzahl gleich. 

```{r cart model ds2}
set.seed(12345)

cart_ds2 <- train(x = train2[,4:41],
                  y = train2$Sepsis,
                  method = "rpart",
                  metric = "F",
                  trControl = ctrl)

```

Aus dem CART-Algorithmus ergibt sich das beste Modell mit: 
`r knitr::kable(cart_ds2$bestTune, row.names = FALSE)`

```{r cart model results ds2, include = TRUE, fig.cap="\\label{cart_ds2}Entscheidungsbaum Onset-Datensatz"}
rpart.plot(cart_ds2$finalModel, type = 5, extra = 108, box.palette = "RdBu")
```

Ähnlich wie beim Baseline-Ansatz wird hier nur eine Prädiktorvariable verwendet - in diesem Fall die Länge des Intensivaufenthalts (*ICULOS*). Analog zum Vorgehen im
vorigen Abschnitt wird nun auch hier diese Variable ausgeschlossen. Dies führt zu:

```{r cart model adjusted ds2}
set.seed(12345)

cart_adj_ds2 <- train(x = train2[,4:40],
                  y = train2$Sepsis,
                  method = "rpart",
                  metric = "F",
                  trControl = ctrl)

```

```{r cart model adjusted results ds2, include = TRUE, fig.cap="Entscheidungsbaum Onset-Datensatz angepasst"}
rpart.plot(cart_adj_ds2$finalModel, type = 5, extra = 108, box.palette = "RdBu")
```

Es folgen die Random Forests - zunächst mit Imputation nach Breiman.

```{r rf model ds2}
set.seed(12345)

# Imputation nach Breiman:

train2_imputed <- rfImpute(train2$Sepsis ~ ., train2) %>%
  dplyr::rename(Sepsis = `train2$Sepsis`) %>%
  relocate(Sepsis, .after = ICULOS)

rf_ds2 <- train(x = train2_imputed[,4:41],
                y = train2$Sepsis,
                method = "ranger",
                tuneGrid = rf_grid,
                num.trees = 100,
                importance = "impurity",
                metric = "F",
                trControl = ctrl)
```

Das beste Modell entsteht mit: 
`r knitr::kable(rf_ds2$bestTune, row.names = FALSE)`

Nun folgt die Einschränkung auf Prädiktoren mit weniger als 30 % fehlenden Werten
und vollständige Beobachtungen.

```{r non-missings ds2, include = TRUE, fig.cap="Fehlende Werte im Onset-Datensatz"}
vis_miss(dataset2) +
  theme(axis.text.x =  element_text(angle = 90))
```

```{r rf non-missing ds2}
set.seed(12345)

train2_2 <- train2[, which(colMeans(!is.na(train2)) > 0.7)] 

# Entsprechend muss mtry im TuneGrid angepasst werden:
n_var2 <- train2_2 %>% 
  select(-Patient_ID, -X, -Hour, -Sepsis) %>% 
  ncol()
  
rf_grid2 <- expand.grid(
  .mtry = seq(2, n_var2 - 1, 3),
  .splitrule = "gini",
  .min.node.size = seq(5, 20, 5)
)

rf_nm_ds2 <- train(Sepsis ~ . - Patient_ID - X - Hour - Sepsis,
                  data = train2_2,
                  method = "ranger",
                  na.action = na.omit,
                  tuneGrid = rf_grid2,
                  num.trees = 100,
                  importance = "impurity",
                  metric = "F",
                  trControl = ctrl)
```

Das beste Modell hat die Parameter: 
`r knitr::kable(rf_nm_ds2$bestTune, row.names = FALSE)`

Und für die Gradient Boosting Machine ergibt sich schließlich:

```{r gbm model ds2}
set.seed(12345)

gbm_ds2 <- train(x = train2[,4:41],
                 y = train2$Sepsis,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "F",
                 tuneGrid = gbm_grid,
                 trControl = ctrl,
                 verbose = FALSE)
```

 `r knitr::kable(gbm_ds2$bestTune, row.names = FALSE)`

## Der Regressionsansatz

Auch für den Regressionsdatensatz werden die gleichen Verfahren und Tuningparameter
benutzt. Mit dem CART-Algorithmus ergibt sich:

```{r cart model ds3}
set.seed(12345)

cart_ds3 <- train(x = train3[,3:40],
                  y = train3$Sepsis,
                  method = "rpart",
                  metric = "F",
                  trControl = ctrl)
```

 `r knitr::kable(cart_ds3$bestTune, row.names = FALSE)`

```{r cart model results ds3, include = TRUE, fig.cap="\\label{cart_ds3}Entscheidungsbaum Regressionsdatensatz"}
rpart.plot(cart_ds3$finalModel, type = 5, extra = 108, box.palette = "RdBu")
```

Wiederum wird im finalen Modell nur die Dauer des Intensivaufenthalts als
Splitvariable verwendet und daher ein weiterer Baum ohne diesen Prädiktor
angepasst:

```{r cart model adjusted ds3}
set.seed(12345)

cart_adj_ds3 <- train(x = train3[,-c(1,2,8)],
                  y = train3$Sepsis,
                  method = "rpart",
                  metric = "F",
                  trControl = ctrl)
```

```{r cart model adjusted results ds3, include = TRUE, fig.cap="\\label{cart_adj_ds3}Entscheidungsbaum Regressionsdatensatz angepasst"}
rpart.plot(cart_adj_ds3$finalModel, type = 5, extra = 108, box.palette = "RdBu")
```

Random Forests mit Imputation führen zu dieser optimalen Parameterwahl:

```{r rf model ds3}
set.seed(12345)

# Imputation nach Breiman:

train3_imputed <- rfImpute(train3$Sepsis ~ ., train3) %>%
  dplyr::rename(Sepsis = `train3$Sepsis`)

rf_ds3 <- train(x = train3_imputed[,3:40],
                y = train3$Sepsis,
                method = "ranger",
                tuneGrid = rf_grid,
                num.trees = 100,
                importance = "impurity",
                metric = "F",
                trControl = ctrl)
```

`r knitr::kable(rf_ds3$bestTune, row.names = FALSE)`

Random Forests nach Einschränkung der Prädiktoren und auf vollständige
Beobachtungen ergeben:

```{r non-missings ds3, include = TRUE, fig.cap="fehlende Werte im Regressionsdatensatz"}
vis_miss(dataset3) +
  theme(axis.text.x =  element_text(angle = 90))
```

```{r rf non-missing ds3}
set.seed(12345)

train3_2 <- train3[, which(colMeans(!is.na(train3)) > 0.7)]

# Entsprechend muss mtry im TuneGrid angepasst werden:
n_var3 <- train3_2 %>% 
  select(-Patient_ID, -Sepsis) %>% 
  ncol()
  
rf_grid3 <- expand.grid(
  .mtry = seq(2, n_var3 - 1, 3),
  .splitrule = "gini",
  .min.node.size = seq(5, 20, 5)
)

rf_nm_ds3 <- train(Sepsis ~ . - Patient_ID - Sepsis,
                  data = train3_2,
                  method = "ranger",
                  na.action = na.omit,
                  tuneGrid = rf_grid3,
                  num.trees = 100,
                  importance = "impurity",
                  metric = "F",
                  trControl = ctrl)
```

`r knitr::kable(rf_nm_ds3$bestTune, row.names = FALSE)`

Und Gradient Boosting führt hier zu den Parametern:

```{r gbm model ds3}
set.seed(12345)

gbm_ds3 <- train(x = train3[,3:40],
                 y = train3$Sepsis,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "F",
                 tuneGrid = gbm_grid,
                 trControl = ctrl,
                 verbose = FALSE)
```

`r knitr::kable(gbm_ds3$bestTune, row.names = FALSE)`

\newpage

## Vergleich der Modelle

```{r resamples}
results <- resamples(list(CART_Baseline = cart_ds1,
                          RF_imputiert_Baseline = rf_ds1,
                          RF_nonmissing_Baseline = rf_nm_ds1,
                          GBM_Baseline = gbm_ds1,
                          CART_Onset = cart_ds2,
                          RF_imputiert_Onset = rf_ds2,
                          RF_nonmissing_Onset = rf_nm_ds2,
                          GBM_Onset = gbm_ds2,
                          CART_Regression = cart_ds3,
                          RF_imputiert_Regression = rf_ds3,
                          RF_nonmissing_Regression = rf_nm_ds3,
                          GBM_Regresion = gbm_ds3))
```

Die folgenden Übersichten vergleichen anhand der Resampling-Statistiken jeweils
den CART-Entscheidungsbaum, den Random Forest nach Imputation, den Random Forest
auf vollständigen Beobachtungen und die Gradient Boosting Machine für jeden 
der drei Datensätze (Baseline, Onset und Regression). Die weiteren angepassten
Entscheidungsbäume, bei denen jeweils die zunächst verwendete Splitvariable aus 
der Betrachtung entfernt wurde, werden nicht mit einbezogen, da sie weniger der
Verbesserung der Vorhersagequalität als vielmehr dem Auffinden relevanter
Kriterien dienen sollen.

```{r resamples F statistics, include = TRUE}
knitr::kable(summary(results)$statistics$F, 
             booktabs = T, 
             caption = "F-Metrik") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"),
                position = "center")
```

```{r resamples AUC statistics, include = TRUE}
knitr::kable(summary(results)$statistics$AUC, 
             booktabs = T, 
             caption = "AUC") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"),
                position = "center")
```

```{r resamples precision statistics, include = TRUE}
knitr::kable(summary(results)$statistics$Precision, 
             booktabs = T, 
             caption  = "Precision") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"),
                position = "center")
```

```{r resamples recall statistics, include = TRUE}
knitr::kable(summary(results)$statistics$Recall, 
             booktabs = T, 
             caption  = "Recall") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"),
                position = "center")
```

\clearpage

```{r resamples plot, include = TRUE, fig.cap="Modellvergleich"}
dotplot(results, cex.axis = 1)
```

Es stellt sich heraus, dass gemessen an der F1-Metrik die Gradient Boosting 
Machine auf dem Regressinsdatensatz das erfolgreichste Modell ist, gefolgt von 
dem CART-Modell und dem Random Forest nach Imputation auf dem gleichen Datensatz.
\newline
Die Modelle für den Baseline-Datensatz erreichen die schlechtesten Ergebnisse. 
Dies ist wenig überraschend, wenn man bedenkt, dass zur Baseline der größte
zeitliche Abstand zum Event besteht. 
\newline
Auch der Erfolg des Regressionsdatensatzes leuchtet ein, da hier die meisten
Informationen eingehen. Der Ansatz, nicht die Absolutwerte, sondern deren
Veränderung zu betrachten, ähnelt im Übrigen der Sepsis-3-Definition, die ebenso
die Veränderung im SOFA-Score, nicht den Absolutwert berücksichtigt.
\newline
Unerwartet kommt hingegen das bessere Abschneiden des einzelnen
Entscheidungsbaumes gegenüber den Random Forests. Allerdings ist der Unterschied 
zwischen dem Entscheidungsbaum und dem Random Forest nach Imputation auf dem 
Regressionsdatensatz sehr gering (Differenz der mittleren F1-Metrik 
`r prettyNum(round(summary(results)$statistics$F[9,4] -
                   summary(results)$statistics$F[10,4], 
                  3),
            decimal.mark = ",")`). 
Außerdem kommt hier sicherlich die Stärke des CART-Algorithmus im Umgang mit
fehlenden Werten durch die Verwendung von Surrogate Splits zum Tragen. Insbesondere
bei Vorliegen eines MNAR-Mechanismus, der hier wie weiter oben erläutert nicht
ausgeschlossen werden kann, sind vergleichsweise schlechte Ergebnisse aufgrund 
einer Verzerrung nach Imputation oder bei einer Complete Case Analysis zu erwarten.
\newline
Zu beachten ist außerdem, dass für die Random Forests des Baseline-Datensatzes 
kein Konfidenzintervall der F1-Metrik angegeben werden kann, da die Metrik auf 
vier der fünf Folds nicht berechnet werden konnte, weil Precision und Recall
jeweils Null sind.

Gemessen an der AUC-Metrik (bezogen auf die Precision Recall-Kurve) ergibt sich 
ein ähnliches Bild. Der deutlichste Unterschied besteht im Ranking des CART-Modells
des Regressionsdatensatzes. Dieses fällt von Platz 2 auf Platz 8. Hier wirkt sich 
aus, dass die Precision im Vergleich zu den anderen Modellen eher schlecht, der 
Recall aber sehr gut ist (mit 
`r prettyNum(round(summary(results)$statistics$Recall[9,4], 3), 
            decimal.mark = ",")`
der zweitbeste Wert).

Von den untersuchten Modellen empfiehlt sich demzufolge vor allem die Gradient
Boosting Machine in Verbindung mit einem Regressionsansatz.

Wird ein weniger rechenintensives oder besser interpretierbares Verfahren benötigt,
so stellt der CART-Baum auf dem Regressionsdatensatz insbesondere dann eine gute
Alternative dar, wenn der Precision im Vergleich zum Recall eher wenig Bedeutung
beigemessen wird. Dies ist zum Beispiel denkbar, wenn durch den Algorithmus eine 
Vorauswahl an gefährdeten Patienten getroffen werden soll, die dann noch einmal
gesondert ärztlich beurteilt wird.

Falls es nicht möglich ist, Messwerte aus einem längeren Zeitraum in die Analyse
einzubeziehen, so ist der auf vollständige Daten eingeschränkte Random Forest auf
dem Onset-Datensatz das Modell der Wahl, dicht gefolgt von der Gradient Boosting
Machine und dem Random Forest mit Imputation auf demselben Datensatz. Je nach 
Vollständigkeit der Daten können sich hier leicht Verschiebungen ergeben, sodass
für Krankenhäuser mit unterschiedlicher Datengrundlage eventuell unterschiedliche 
Modelle am besten geeignet sind.

Von einer Verwendung des Baseline-Datensatzes ist grundsätzlich abzuraten, da 
keines der evaluierten Modelle eine befriedigende Performance zeigt (bestes Modell
GBM mit F1 = 
`r prettyNum(round(summary(results)$statistics$F[4,4], 3), decimal.mark = ",")`,
Precision = 
`r prettyNum(round(summary(results)$statistics$Precision[4,4], 3), 
            decimal.mark = ",")` 
und Recall = 
`r prettyNum(round(summary(results)$statistics$Recall[4,4], 3), 
             decimal.mark = ",")`).

Idealerweise würde man die Modelle nun an einem komplett unabhängigen Testdatensatz
(d. h. Daten aus einem anderen Krankenhaus und möglichst auch anderem Quellsystem)
testen. Hier wird auf die eingangs abgespaltenen Testdatensätze mit 25 % der
Beobachtungen des ursprünglichen Samples zurückgegriffen - in dem Bewusstsein, 
dass diese aus den gleichen Datenquellen stammen wie die Trainingsdaten. 
\newline Hinsichtlich der Random Forests mit imputierten Daten stellt sich die
Frage, wie die Imputation der Testdaten erfolgen soll. *rfImpute* setzt voraus,
dass die Daten gelabelt sind. Dies ist zwar bei den Testdaten erfüllt, stellt aber
kein realistisches Szenario dar. Bei einem realen Einsatz des Modells sind keine
Label bekannt, sodass dieses Vorgehen nicht übertragbar wäre. Hier werden daher 
alle Beobachtungen für die Imputation vorübergehend mit einem negativen Label
versehen, d. h. mit dem bisher angenommenen Status (in dem Sinne, dass bereits 
als Sepsisfälle diagnostizierte Patienten nicht mehr klassifiziert werden müssten).
Dann wird, wie auch beim Training des Modells, *rfImpute* angewendet. Alternativ 
wären Imputataionsverfahren möglich, die kein Label voraussetzen.

Die Tests führen zu folgenden Konfusionsmatrizen:

```{r model test confusion matrices}
pred_cart_ds1 <- predict(cart_ds1, test1)

# speichern als png nötig für Skalierung der  Statistikbox
png("figures/cf_plot1.png")   
binary_visualiseR(train_labels = pred_cart_ds1,
                               truth_labels = test1$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "CART",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Für Random Forest zunächst Testdaten imputieren
set.seed(12345)

test1_imputed <- test1 %>% 
  mutate(Sepsis0 = as.factor("negativ")) %>% 
  rbind(., cbind(train1[1,], Sepsis0 = as.factor("positiv")))
        # es muss eine zweite Klasse geben

test1_imputed <- rfImpute(Sepsis0 ~. - Sepsis , test1_imputed) %>% 
  filter(Sepsis0 == "negativ") %>%     # Trainingsdaten wieder entfernen
  select(-Sepsis0)

pred_rf_ds1 <- predict(rf_ds1, test1_imputed[-42])

png("figures/cf_plot2.png")
binary_visualiseR(train_labels = pred_rf_ds1,
                               truth_labels = test1_imputed$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "Random Forest (imputiert)",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Vorhersagen nur für vollständige Beobachtungen
test1_2 <- test1[, colnames(train1_2)] %>%
  filter(complete.cases(.))

pred_rf_nm_ds1 <- predict(rf_nm_ds1, test1_2)

png("figures/cf_plot3.png")
binary_visualiseR(train_labels = pred_rf_nm_ds1,
                               truth_labels = test1_2$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "Random Forest (non-missing)",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

pred_gbm_ds1 <- predict(gbm_ds1, test1)

png("figures/cf_plot4.png")
binary_visualiseR(train_labels = pred_gbm_ds1,
                               truth_labels = test1$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "GBM",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

pred_cart_ds2 <- predict(cart_ds2, test2)

png("figures/cf_plot5.png")
binary_visualiseR(train_labels = pred_cart_ds2,
                               truth_labels = test2$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "CART",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Testdaten imputieren
set.seed(12345)

test2_imputed <- test2 %>% 
  mutate(Sepsis0 = as.factor("negativ")) %>% 
  rbind(., cbind(train2[1,], Sepsis0 = as.factor("positiv")))
        # es muss eine zweite Klasse geben

test2_imputed <- rfImpute(Sepsis0 ~. - Sepsis , test2_imputed) %>% 
  filter(Sepsis0 == "negativ") %>%     # Trainingsdaten wieder entfernen
  select(-Sepsis0)

pred_rf_ds2 <- predict(rf_ds2, test2_imputed[-42])

png("figures/cf_plot6.png")
binary_visualiseR(train_labels = pred_rf_ds2,
                               truth_labels = test2_imputed$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "Random Forest (imputiert)",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Vorhersagen nur für vollständige Beobachtungen
test2_2 <- test2[, colnames(train2_2)] %>%
  filter(complete.cases(.))

pred_rf_nm_ds2 <- predict(rf_nm_ds2, test2_2)

png("figures/cf_plot7.png")
binary_visualiseR(train_labels = pred_rf_nm_ds2,
                               truth_labels = test2_2$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "Random Forest (non-missing)",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

pred_gbm_ds2 <- predict(gbm_ds2, test2)

png("figures/cf_plot8.png")
binary_visualiseR(train_labels = pred_gbm_ds2,
                               truth_labels = test2$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "GBM",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

pred_cart_ds3 <- predict(cart_ds3, test3)

png("figures/cf_plot9.png")
binary_visualiseR(train_labels = pred_cart_ds3,
                               truth_labels = test3$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "CART",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Testdaten imputieren
set.seed(12345)

test3_imputed <- test3 %>% 
  mutate(Sepsis0 = as.factor("negativ")) %>% 
  rbind(., cbind(train3[1,], Sepsis0 = as.factor("positiv")))
        # es muss eine zweite Klasse geben

test3_imputed <- rfImpute(Sepsis0 ~. - Sepsis , test3_imputed) %>% 
  filter(Sepsis0 == "negativ") %>%     # Trainingsdaten wieder entfernen
  select(-Sepsis0)

pred_rf_ds3 <- predict(rf_ds3, test3_imputed[-2])

png("figures/cf_plot10.png")
binary_visualiseR(train_labels = pred_rf_ds3,
                               truth_labels = test3_imputed$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "Random Forest (imputiert)",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Vorhersagen nur für vollständige Beobachtungen
test3_2 <- test3[, colnames(train3_2)] %>%
  filter(complete.cases(.))

pred_rf_nm_ds3 <- predict(rf_nm_ds3, test3_2)

png("figures/cf_plot11.png")
binary_visualiseR(train_labels = pred_rf_nm_ds3,
                               truth_labels = test3_2$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "Random Forest (non-missing)",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

pred_gbm_ds3 <- predict(gbm_ds3, test3)

png("figures/cf_plot12.png")
binary_visualiseR(train_labels = pred_gbm_ds3,
                               truth_labels = test3$Sepsis,
                               class_label1 = "positiv", 
                               class_label2 = "negativ",
                               quadrant_col1 = "#E5826D",
                               quadrant_col2 = "#578C99",
                               custom_title = "GBM",
                               info_box_title = "Statistik",
                               text_col = "black")
dev.off()

# Dataframe der Konfusionsmatrix für spätere Verwendung (sofern benötigt)
cf_gbm_ds3 <- binary_class_cm(pred_gbm_ds3, test3$Sepsis)

# zur Anwendung von grid.arrange in grob-Objekt konvertieren
cf_table1 <- rasterGrob(readPNG("figures/cf_plot1.png"))
cf_table2 <- rasterGrob(readPNG("figures/cf_plot2.png"))
cf_table3 <- rasterGrob(readPNG("figures/cf_plot3.png"))
cf_table4 <- rasterGrob(readPNG("figures/cf_plot4.png"))
cf_table5 <- rasterGrob(readPNG("figures/cf_plot5.png"))
cf_table6 <- rasterGrob(readPNG("figures/cf_plot6.png"))
cf_table7 <- rasterGrob(readPNG("figures/cf_plot7.png"))
cf_table8 <- rasterGrob(readPNG("figures/cf_plot8.png"))
cf_table9 <- rasterGrob(readPNG("figures/cf_plot9.png"))
cf_table10 <- rasterGrob(readPNG("figures/cf_plot10.png"))
cf_table11 <- rasterGrob(readPNG("figures/cf_plot11.png"))
cf_table12 <- rasterGrob(readPNG("figures/cf_plot12.png"))
```

```{r confusion matrix plots1, include = TRUE, out.height='150%', out.width='120%', fig.cap="Konfusionsmatrizen (Baseline-Datensatz)"}
grid.arrange(cf_table1, cf_table2, cf_table3, cf_table4,
             ncol = 2, nrow = 2)
```

```{r confusion matrix plots2, include = TRUE, out.height='150%', out.width='120%', fig.cap="Konfusionsmatrizen (Onset-Datensatz)"}
grid.arrange( cf_table5, cf_table6, cf_table7, cf_table8, 
             ncol = 2, nrow = 2)
```

```{r confusion matrix plots3, include = TRUE, out.height='150%', out.width='120%', fig.cap="Konfusionsmatrizen (Regressionsdatensatz)"}
grid.arrange(cf_table9, cf_table10, cf_table11, cf_table12,
             ncol = 2, nrow = 2)
```

Fast alle Modelle weisen hohe Accuracys von über 90 % auf (Ausnahmen: CART, Random
Forest (imputiert) und GBM auf dem Baseline-Datensatz). Allerdings ist dies
aufgrund der niedrigen Prävalenz wenig aussagekräftig, da Werte in diesem Bereich
bereits erreicht würden, wenn immer negativ klassifiziert würde.

Da wir uns mit den Sepsis-Labeln zeitlich vor der tatsächlichen Diagnose befinden,
kann grundsätzlich jede richtig positive Klassifizierung als Verbesserung 
verstanden werden. Aufgrund des binären Outcomes wäre eine Sensitivität von etwa 
50 % zwar auch bei zufälliger Klassifizierung zu erwarten, allerdings würde dieses
Vorgehen zu einer Spezifität von ebenfalls nur ca. 50 % führen. Man kann also
festhalten, dass ein Modell erfolgreich ist, wenn es eine Sensitivität von deutlich
über 0 bei gleichzeitig "hoher" Spezifität erreicht. Die Frage, wann die Spezifität
hoch genug ist, lässt sich nicht eindeutig beantworten. Ein entsprechender
Grenzwert sollte daher in jeder Anwendungssituation individuell mit den beteiligten
Fachspezialisten festgelegt werden. Als Mindestanforderung sollte aber gelten, dass
die Zahl der richtig positiven Ergebnisse (TP) höher als die der falsch positiven
(FP) ist, d. h. dass mehr Patienten frühzeitig einer tatsächlich notwendigen
Behandlung zugeführt werden als aufgrund falscher Klassifizierung potenziell
übertherapiert werden.

Anhand dieser Kriterien sind folgende Modelle eindeutig auszuschließen:

- CART Baseline
- Random Forest (imputiert) Baseline
- Random Forest (non-missing) Baseline
- GBM Baseline
- Random Forest (imputiert) Onset

Im Vergleich zu den Ergebnissen der Kreuzvalidierung fällt die deutlich schlechtere
Sensitivität des Random Forests auf dem imputierten Onset-Datensatz auf, aufgrund
derer dieses Modell nun aus den Empfehlungen herausfällt.

Hinsichtlich des bisherigen Favoriten, der Gradient Boosting Machine für den 
Regressionsdatensatz, ist festzuhalten, dass das Verhältnis FP/TP mit
`r prettyNum(
            round(cf_gbm_ds3$cm_tbl[3,2]/cf_gbm_ds3$cm_tbl[1,2], 2), 
            decimal.mark = ",")` 
vergleichsweise hoch ist. Unter diesem Aspekt könnte der Random Forest nach
Imputation auf dem Regressionsdatensatz trotz seiner deutlich niedrigeren
Sensitivität je nach Anwendungssituation vorzuziehen sein. Der Random Forest auf
vollständigen Daten stellt in dieser Hinsicht einen Kompromiss dar, ist allerdings
nur auf neue Daten anwendbar, wenn Vollständigkeit hinsichtlich der einbezogenen
Variablen erreicht werden kann.

## Entscheidungsregeln und Einfluss einzelner Prädiktoren

Die Plots am Ende dieses Abschnitts stellen die Variablenwichtigkeit der einzelnen
Modelle dar. Dabei erhält die jeweils wichtigste Variable eine Score von 100 und 
die weiteren Variablen werden in Relation dazu bewertet. Kriterium ist die
Verbesserung hinsichtlich des Gini-Unreinheitsmaßes.

Es bestätigt sich, was schon anhand der weiter oben visualisierten Bäume abzusehen
war: Den mit Abstand größten Einfluss auf die Klassifizierung hat die Dauer des 
Intensivaufenthalts (*ICULOS*) beziehungsweise zur Baseline, wenn *ICULOS* noch 
wenig aussagekräftig ist, da die Intensivbehandlung meistens gerade erst begonnen
hat, die Zeit, die zwischen Krankenhausaufnahme und Verlegung auf die
Intensivstation vergangen ist (*HospAdmTime*). Dies gilt für alle Modelle außer
dem Random Forest nach Imputation beim Onset-Ansatz; dort hat *Bilirubin_total*
die größte Wichtigkeit.

Aus den Splits in den Abbildungen \ref{cart_ds2} und \ref{cart_ds3} wird
ersichtlich, dass zumindest nach den hier vorliegenden Daten das Risiko, eine 
Sepsis zu entwickeln, steigt, wenn ein Intensivaufenthalt länger als etwa 2,5 Tage
andauert. Und auch innerhalb der ersten 20 Stunden (*ICULOS* < 14 plus sechs 
Stunden vom Label bis zur Diagnose) scheint eine Sepsisdiagnose wahrscheinlich.
Vermutlich handelt es sich hierbei um Patienten, die bereits mit einer Sepsis auf
die Intensivstation kommen, jedoch noch nicht unbedingt diagnostiziert sind,
während die zuerst genannte Gruppe die Sepsis erst auf der Intensivstation erwirbt.
Dennoch muss der Intensivaufenthalt nicht notwendigerweise kausal für die
Erkrankung sein. Möglich ist zum Beispiel auch, dass Personen mit einem längeren
Intensivaufenthalt schwerer krank und somit in einem schlechteren Allgemeinzustand
sind, der das Entstehen einer Sepsis begünstigt. Ähnlich könnte auch Abbildung
\ref{cart_ds1} interpretiert werden: Wer sehr schnell (innerhalb der ersten
eineinhalb Stunden) oder erst spät (nach mehr als 45 Stunden) auf die
Intensivstation verlegt wurde, ist in einem schlechteren Zustand als die übrigen
Intensivpatienten und hat möglicherweise deshalb ein höheres Sepsisrisiko.

Die große Bedeutung der beiden Prädiktoren kann auch stark davon beeinflusst sein,
dass es sich um zwei von insgesamt nur vier (zusammen mit Alter und Geschlecht) 
vollständigen Variablen handelt.

Darüber hinaus ergibt sich kein eindeutiges Bild, wenn man sich die Abbildungen
der Variablenwichtigkeit und der Entscheidungsbäume ansieht. Insgesamt scheint 
den Vitalparametern (Blutdruckwerte, Herz- und Atemfrequenz, Temperatur und
Sauerstoffsättigung) eine größere Bedeutung zuzukommen als den Blutwerten. Jedoch
gilt auch für diese Werte, dass sie vergleichsweise häufig verfügbar sind.

Im Folgenden werden die beiden favorisierten Modelle genauer angesehen:

Die Gradient Boosting Machine mit dem Regressionsansatz wird stärker als jedes
andere Modell von *ICULOS* dominiert. Kein anderer Prädiktor erreicht einen Score
über 10 und die nächstplatzierten Variablen liegen sehr dicht beisammen. Eine
sinnvolle Interpretation einzelner Bäume der GBM ist aufgrund des iterativen 
Aufbaus des Algorithmus kaum möglich. Insgesamt lässt sich daher nur sagen, dass
eine große Bandbreite an Prädiktoren Verwendung findet, der Dauer des Aufenthalts
auf der Intensivstation aber die mit Abstand größte Bedeutung zukommt.

Bei dem auf dem Testdatensatz wegen seiner sehr hohen Spezifität positiv
aufgefallenen Modell, dem Random Forest nach Imputation auf dem
Regressionsdatensatz, zeichnet sich ein klareres Bild ab. Hier ist der Slope des
endtidalen Kohlendioxids (*EtCO2*) die mit Abstand zweitwichtigste Variable.
Zu beachten ist auch, dass an dritter Stelle der Slope des *Bilirubin_total* 
folgt (der Variable, die im Random Forest des Onset-Datensatzes als wichtigste
bestimmt wurde), denn wie aus Abbildung \ref{cor_plot} ersichtlich ist, korreliert
*Bilirubin_total* vollständig mit dem aus unseren Daten entfernten
*Bilirubin_direct* und letzteres korreliert deutlich negativ mit *EtCO2*. Der
Korrelationskoeffizient für *EtCO2* und *Bilirubin_total* selbst ist wenig
aussagekräftig, weil es kaum Beobachtungen gibt, die für beide Variablen Werte
aufweisen. Wie im Kapitel zur Datenexploration bereits erwähnt, ist es möglich,
dass für einen Split nicht die eigentlich relevante Variable herangezogen wird,
sondern eine mit ihr korrelierende. Das könnte bedeuten, dass nur einer der beiden
hier diskutierten Prädiktoren wirklich bedeutsam für den Random Forest ist, dies
aber in umso höherem Maße. Falls dies zutrifft, ist zu erwarten, dass entweder 
ein negativer *EtCO2*-Slope (Verringerung des Werts) und ein positiver
*Bilirubin_total*-Slope (Erhöhung des Werts) die Vorhersagewahrscheinlichkeit 
einer Sepsis erhöhen oder genau andersherum. Um dies zu untersuchen, soll ein
Vorhersage-Interaktions-Plot der beiden Variablen auf dem Modell betrachtet 
werden. Eine Funktion, um einen solchen Plot zu erstellen, ist im R-Paket
*randomForestExplainer* enthalten. Allerdings funktioniert diese nicht für
*ranger*-Objekte, die aus einem *caret::train*-Objekt extrahiert wurden. Daher
wurde der Code hier leicht modifiziert. Die Änderungen befinden sich in den
Codezeilen 12, 13 und 44 (wobei die Änderung in Zeile 44 nicht zwingend notwendig
ist):

```{r plot_predict_interaction function, attr.source = ".numberLines", include = TRUE, echo = TRUE}
plot_predict_interaction.caret_ranger <-
function(forest, data, variable1, variable2, grid = 100, 
          main = paste0("Prediction of the forest for different values of ", 
                 paste0(variable1, paste0(" and ", variable2))), time = NULL) 
{
  newdata <- expand.grid(seq(min(data[[variable1]]), max(data[[variable1]]), 
                             length.out = grid), 
                         seq(min(data[[variable2]]), max(data[[variable2]]), 
                            length.out = grid))
  colnames(newdata) <- c(variable1, variable2)
      # for ranger model extracted from caret::train object
  if (as.character(forest$call)[[1]] == "ranger::ranger") { 
    other_vars <- forest$x  # all but outcome
  }
  else if (as.character(forest$call[[2]])[3] == ".") {     
    other_vars <- setdiff(names(data), as.character(forest$call[[2]])[2])
  }
  else {
    other_vars <- labels(terms(as.formula(forest$call[[2]])))
  }
 
  other_vars <- setdiff(other_vars, c(variable1, variable2))
  n <- nrow(data)
  for (i in other_vars) {
    newdata[[i]] <- data[[i]][sample(1:n, nrow(newdata), 
                                     replace = TRUE)]
  }
  if (forest$treetype == "Regression") {
    newdata$prediction <- 
      predict(forest, newdata, type = "response")$predictions
    plot <- ggplot(newdata, aes_string(x = variable1, y = variable2, 
                                       fill = "prediction")) + 
      geom_raster() + 
      theme_bw() + 
      scale_fill_gradient2(midpoint = min(newdata$prediction) + 
                             0.5 * (max(newdata$prediction) - 
                                   min(newdata$prediction)), 
                           low = "blue", high = "red")
  }
  else if (forest$treetype == "Probability estimation") {
    id_vars <- colnames(newdata)
    pred <- predict(forest, newdata)$predictions
    if (ncol(pred) == 2) {
      newdata[, paste0("probability_", colnames(pred)[1])] <- pred[,1] 
    }                               #changed from -1 to 1 to get positives
    else {
      newdata[, paste0("probability_", colnames(pred))] <- pred
    }
    newdata <- reshape2::melt(newdata, id.vars = id_vars)
    newdata$prediction <- newdata$value
    plot <- ggplot(newdata, aes_string(x = variable1, y = variable2, 
                                       fill = "prediction")) + 
      geom_raster() + 
      theme_bw() + 
      facet_wrap(~variable) + 
      scale_fill_gradient2(midpoint = min(newdata$prediction) + 
                                      0.5 * (max(newdata$prediction) 
                                            - min(newdata$prediction)), 
        low = "blue", high = "red")
  }
  else if (forest$treetype == "Classification") {
    stop(
      "Ranger forest for classification needs to be generated by ranger(...,
      probability = TRUE).")
  }
  else if (forest$treetype == "Survival") {
    pred <- predict(forest, newdata, type = "response")
    if (is.null(time)) {
      time <- pred$unique.death.times[which.min(abs(colMeans(pred$survival, 
                                                    na.rm = TRUE) - 0.5))]
      message(sprintf(
        "Using unique death time %s which is the closest to predicted median
        survival time.", 
                      time))
    }
    else if (!time %in% pred$unique.death.times) {
      new_time <- 
        pred$unique.death.times[which.min(
                                  abs(pred$unique.death.times - time))]
      message(sprintf("Using closest unique death time %s instead of %s.", 
                      new_time, time))
      time <- new_time
    }
    newdata$prediction <- pred$survival[, pred$unique.death.times == time, 
                                        drop = TRUE]
    plot <- ggplot(newdata, aes_string(x = variable1, y = variable2, 
                                       fill = "prediction")) + 
      geom_raster() + 
      theme_bw() + 
      scale_fill_gradient2(midpoint = min(newdata$prediction) + 
                                      0.5 * (max(newdata$prediction) 
                                             -  min(newdata$prediction)), 
                           low = "blue", high = "red")
  }
  else {
    stop(sprintf("Ranger forest type '%s' is currently not supported.", 
                 forest$treetype))
  }
  if (!is.null(main)) {
    plot <- plot + ggtitle(main)
  }
  return(plot)
}
```

```{r interaction plot, include = TRUE, fig.cap="Vorhersage-Interaktion zwischen Bilirubin und Kohlendioxidwert im Random Forest (Regressionsdatensatz)"}
plot_predict_interaction.caret_ranger(rf_ds3$finalModel,train3_imputed, 
                                      "slope_Bilirubin_total", "slope_EtCO2",
                                      main = "")
```

Tatsächlich ist zu erkennen, dass die Vorhersagewahrscheinlichkeit für eine Sepsis
am höchsten ist, wenn der *Bilirubin_total*-Wert steigt und der *EtCO2*-Wert
abnimmt.
\newline
Medizinisch wird eine Abnahme des endtidalen Kohlendioxidwerts eher mit einer 
Hyperventilation in der Frühphase einer Sepsis in Verbindung gebracht, ein
Anstieg des Bilirubin-Werts hingegen mit einem Leberversagen in der Spätphase der 
Erkrankung. Insofern erscheint Bilirubin zur frühen Detektion von Sepsen
nicht gut geeignet. Stellt man allerdings einen Bilirubinanstieg fest, nachdem 
zuvor schon der Kohlendioxidwert gesunken war, so kann dies als ernster Hinweis
auf eine fortschreitende Sepsis angesehen werden.\footnote{ \label{note1} basierend
auf einer Diskussion mit Dr. med. Ulf Dennler, Leiter der Stabsstelle Strategisches
Medizincontrolling am Universitätsklinikum Ulm, Intensivmediziner, dem ich hiermit
für seinen Beitrag danke.}

Die CART-Bäume wurden hauptsächlich mit dem Ziel angepasst, Entscheidungsregeln
beziehungsweise -pfade abzuleiten. Da die aus dem Tuning entstandenen Bäume nur 
die Dauer des Intensivaufenthalts beziehungsweise die Zeit von der Hospitalisierung
bis zur Verlegung auf die Intensivstation abbilden, ist aus ihnen außer den 
bereits ausgeführten Überlegungen zu diesen Variablen nicht viel ersichtlich. 
Die Auswertungen der Variablenwichtigkeit und die weiteren generierten
Entscheidungsbäume, denen die jeweilige zuvor stärkste Variable entzogen wurde,
lassen jedoch vermuten, dass eine erhöhte Körpertemperatur (*Temp*), ein niedriger 
oder fallender Blutdruck (*MAP*, *DBP*, *SBP*), eine hohe Atemfrequenz (*Resp*)
sowie eine hohe Herzfrequenz (*HR*) Anzeichen für eine beginnende Sepsis sein 
können. Dies deckt sich in etwa mit Teilen der obligaten Diagnosekriterien nach
[@Werdan] und allgemein üblichen SIRS-Kriterien.
\newline
Abbildung \ref{cart_adj_ds3} zeigt außerdem ein komplexes Zusammenspiel
verschiedener Blutwerte, die in einem Zusammenhang mit Entzündungsreaktionen oder 
einem (beginnenden) Organversagen stehen können. Eine Ableitung anwendbarer 
Entscheidungsregeln hieraus, ohne dass eine weitere Differenzierung vorgenommen
wird, ist aber kritisch zu sehen, zumal Veränderungen dieser Werte auch im Kontext
mit Vor- und Begleiterkrankungen betrachtet werden müssen. So ist beispielsweise
eine Änderung des Glukosewerts bei einem Diabetiker anders zu beurteilen als bei
einem Nicht-Diabetiker. Zudem können die Werte erheblich durch medikamentöse
Behandlungen beeinflusst sein, zu denen die Daten aber keine Informationen
enthalten.\footnote{ wie \ref{note1}}

\newpage

```{r variable importance 1, include = TRUE, out.width="250px", fig.show="hold", fig.cap="TOP 20 Variablenwichtigkeit"}
plot(varImp(cart_ds1), top = 20, xlab = "", main = "CART Baseline")
plot(varImp(rf_ds1), top = 20, xlab = "", 
     main = "Random Forest (imputiert) Baseline")
plot(varImp(rf_nm_ds1), top = min(20, n_var1), xlab = "", 
     main = "Random Forest (non-missing) Baseline")
plot(varImp(gbm_ds1), top = 20, xlab = "", main = "GBM Baseline")
plot(varImp(cart_ds2), top = 20, xlab = "", main = "CART Onset")
plot(varImp(rf_ds2), top = 20, xlab = "", 
     main = "Random Forest (imputiert) Onset")
```

\newpage

```{r variable importance 2, include = TRUE, out.width="250px", fig.show="hold", fig.cap="TOP 20 Variablenwichtigkeit (Fortsetzung)"}
plot(varImp(rf_nm_ds2), top = min(20, n_var2), xlab = "", 
     main = "Random Forest (non-missing) Onset")
plot(varImp(gbm_ds2), top = 20, xlab = "", main = "GBM Onset")
plot(varImp(cart_ds3), top = 20, xlab = "", main = "CART Regression")
plot(varImp(rf_ds3), top = 20, xlab = "", 
     main = "Random Forest (imputiert) Regression")
plot(varImp(rf_nm_ds3), top = min(20, n_var3), xlab = "", 
     main = "Random Forest (non-missing) Regression")
plot(varImp(gbm_ds3), top = 20, xlab = "", main = "GBM Regression")
```

\newpage

## Deployment und Testbetrieb

Für den regelmäßigen klinischen Einsatz eines der erstellten Modelle muss zum
einen die  automatiserte Datenbereitstellung sichergestellt werden, d. h. das
Auslesen aus den Quellsystemen (wie zum Beispiel Krankenhaus- und
Laborinformationssystem) und die Datenaufbereitung (je nach Ansatz zum Beispiel
Erstellung des Regressionsdatensatzes oder Imputation). Zum anderen müssen die
Ergebnisse der Klassifizierung in geeigneter Weise verfügbar gemacht werden -
beispielsweise durch die regelmäßige Ausgabe einer Liste aller gefährdeten (d. h.
positiv klassifzierten) Patienten oder mittels einer Oberfläche zur Abfrage der
aktuellen Sepsiswahrscheinlichkeit einzelner Patienten. Idealerweise würden diese 
Informationen in bereits bestehende Anwendungen wie eine interaktive
Visitenliste oder ein Infektionsdashboard eingebunden.

Im Rahmen dieser Arbeit ist eine Shiny-App entstanden, die eine Möglichkeit zur 
Visualisierung der Ergebnisse aufzeigt. Nach Auswahl eines Modells und eines
Patienten werden die Wahrscheinlichkeit für eine Sepsis dieses Patienten und das
zugehörige Break down-Profil angezeigt. Letzteres visualisiert, wie sich die 
durchschnittliche Sepsiswahrscheinlichkeit (bezogen auf die Gesamtheit der
Trainingsdaten) heruntergebrochen auf den additiven Einfluss einzelner
Prädiktoren für den konkreten Patienten erhöht oder verringert (basierend auf 
@Biecek). Außerdem werden die zeitlich unveränderlichen Basisdaten zum Patienten
und die Zeitreihe einer auszuwählenden Variablen eingeblendet.
\newline
Vorausgesetzt werden ein dem verwendeten Ansatz entsprechend aufbereiteter
Datensatz, ein Datensatz mit den Einzelbeobachtungen zu den Patienten, ein
trainiertes Modell und der zugehörige DALEX-Explainer.
\newline
Mit dem R-Paket *DALEX* erzeugte Explainer ermöglichen die Berechnung verschiedener
erklärender Elemente zu unterschiedlichen Modellen. Eines dieser Elemente ist das
beschriebene Break down-Profil.

Durch die Möglichkeit, nicht nur das Ergebnis der Vorhersage 
(Sepsiswahrscheinlichkeit) anzuzeigen, sondern auch in einem gewissen Maße zu
analysieren, wie dieses zustande kommt, ist die App auch für einen Testbetrieb 
zur weiteren Evaluierung und gegebenenfalls Verbesserung der Modelle geeignet. 
Zu diesem Zweck kann es sinnvoll sein, mehrere Modelle zu integrieren, wohingegen
man sich im Produktiveinsatz auf ein Modell beschränken würde.
\newline
In die vorliegende App wurden die Gradient Boosting Machine und der Random Forest
nach Imputation in Verbindung mit dem Regressionsansatz aufgenommen und der 
Testdatensatz eingebunden.

```{r model explainers}
# predict-Funktion zur Berechnung der Break downs
predict.function <- function(model, new_observation) {
  predict(model, new_observation, type = "prob")[,1] # positiv = Event
}

# Explainer für ausgewählte Modelle
explain_gbm_ds3 <- DALEX::explain(gbm_ds3,
                           data = train3[,3:40],
                           y = case_when(train3$Sepsis == "positiv" ~ 1, 
                                         TRUE ~ 0), # benötigt numerischen Outcome
                           label = "GBM")

explain_rf_ds3 <- DALEX::explain(rf_ds3,
                           data = train3_imputed[,3:40],
                           y = case_when(train3_imputed$Sepsis == "positiv" ~ 1, 
                                         TRUE ~ 0), # benötigt numerischen Outcome
                           label = "Random Forest")
```

\newpage

# Zusammenfassung und Ausblick

Wir haben gesehen, dass es sich für eine Sepsisprädiktion anbietet, Blutwerte 
und Vitalparameter aus einem Behandlungsintervall zu berücksichtigen, anstatt nur
Werte zu einem bestimmten Zeitpunkt zu verwenden. Dies kann durch die Betrachtung
der Veränderung dieser Werte in Form von Slopes einer patientenindividuellen
linearen Regression erfolgen.

Mit diesem Ansatz haben alle untersuchten Modelle (CART-Entscheidungsbaum, Random
Forests und Gradient Boosting Machine) ähnliche Ergebnisse hinsichtlich der
F1-Metrik erreicht. Lediglich hinsichtlich der Relevanz ist der CART-Algorithmus
deutlich schlechter.
\newline
Als erfolgversprechendste Modelle zur weiteren Erprobung ergeben sich die
Gradient Boosting Machine und die Random Forests in Verbindung mit dem
beschriebenen Regressionsansatz. Sie haben auf den Testdaten akzeptable
Sensitivitäten zwischen 15 und 55 % und sehr gute Spezifitäten von 98 bis 99,8 %
erreicht.

Hinsichtlich der Frage nach der Ableitbarkeit von Entscheidungsregeln zur nicht
maschinellen Unterstützung der ärztlichen Entscheidungsfindung konnten keine
belastbaren Ergebnisse erzielt werden. Die Splitregeln der Entscheidungsbäume
stützen sich sehr stark auf die Dauer des Intensivaufenthalts, was in dieser
Hinsicht nicht genügend weiterhilft.

Bevor ein klinischer Einsatz der Vorhersagemodelle in Betracht gezogen werden 
kann, müssen die infrage kommenden Modelle nun auf verschiedenen Datensätzen aus
unterschiedlichen Quellen getestet werden, um zu beurteilen, wie gut sich die
Ergebnisse verallgemeinern lassen. Insbesondere die Problematik fehlender Werte
könnte sich auf verschiedenen Datensätzen sehr unterschiedlich auswirken und vor
allem die Random Forests stark beeinflussen.

Unabhängig davon, wie gut sich die Modelle verallgemeinern lassen, ist eine 
weitere Verbesserung der Sensitivität, ohne dass es zu einer deutlichen
Verschlechterung der Spezifität oder Relevanz kommt, erstrebenswert. Zu diesem
Zweck sollten weitere Strategien zum Umgang mit fehlenden Werten erprobt werden - 
zum Beispiel multiple Imputation, aber möglichst auch die Erhöhung des Füllgrads 
zumindest einiger Prädiktoren (beispielsweise im Rahmen von Kooperationen).
\newline
Außerdem ist die Wahl der Evaluationsmetrik eine wichtige Stellschraube, die noch
weitere Möglichkeiten bietet, zum Beispiel die Verwendung einer
verallgemeinerten F-Metrik oder gar die Implementierung eines ganz individuell 
auf die Fragestellung angepassten Evaluationsmaßes.
\newline
Und letztlich gibt es viele weitere Modelle, die grundsätzlich zur binären
Klassifizierung geeignet sind und möglicherweise einen wesentlichen Beitrag zu 
dem untersuchten Thema leisten können. Dies reicht von anderen 
Boosting-Algorithmen bis hin zu verschiedenen nicht baumbasierten Verfahren wie
beispielsweise neuronalen Netzen.

\newpage

# Anhang

Diese Arbeit wurde mit "`r sessioninfo::platform_info()$version`" unter Verwendung
folgender Pakete erstellt:

```{r session info, include = TRUE, cache = FALSE}
knitr::kable(subset(data.frame(sessioninfo::package_info()), 
                    attached == TRUE, 
                    c(loadedversion)),
              col.names = "Version") 
```

```{r vars description, include = TRUE, cache = FALSE}
knitr::kable(var_info, booktabs = T,
             caption = "\\label{vars_table}Datensatzbeschreibung (Variablen)") %>%
  kable_styling(latex_options = c("striped", "scale_down"),
                position = "center") %>% 
  kableExtra::pack_rows("Technische Angaben", 1, 2) %>%
  kableExtra::pack_rows("Vitalparameter", 3, 10) %>%
  kableExtra::pack_rows("Blutwerte", 11, 36) %>% 
  kableExtra::pack_rows("Basisdaten & Outcome", 37, 44) 
```

\newpage

# Referenzen
